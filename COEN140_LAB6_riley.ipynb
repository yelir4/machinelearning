{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "You will now be using the ensemble methods you have just learned about to find the best performance on the [Kaggle Breast Cancer Dataset](https://www.kaggle.com/merishnasuwal/breast-cancer-prediction-dataset), which has already been provided for you in the file `Breast_cancer_data.csv`. The dataset was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg. It contains 5 features and one predictor, titled `diagnosis`, which is either 1 (has breast cancer) or 0 (does not have breast cancer).\n",
    "\n",
    "### 1. Tune Ensemble Models\n",
    "\n",
    "Read the dataset using Pandas and separate the predictor from the other features. Split the dataset using an 80/20 split and keep the test set separate for evaluation. Using what you have learned thus far, tune the best of each of the following models you can and report their F1 score on the test set: Decision Tree, Bagged Decision Tree, Random Forest, and AdaBoost. For each model type, keep track of the best trained model (you'll need it in the following task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "#import necessary utilities\n",
    "print(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breast cancer data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.8</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.1184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.9</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.1096</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.1</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.1003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.2</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.0978</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>16.6</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.3</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>20.6</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.1</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1      2       3        4  5\n",
       "1    17.99  10.38  122.8  1001.0   0.1184  0\n",
       "2    20.57  17.77  132.9  1326.0  0.08474  0\n",
       "3    19.69  21.25  130.0  1203.0   0.1096  0\n",
       "4    11.42  20.38  77.58   386.1   0.1425  0\n",
       "5    20.29  14.34  135.1  1297.0   0.1003  0\n",
       "..     ...    ...    ...     ...      ... ..\n",
       "565  21.56  22.39  142.0  1479.0    0.111  0\n",
       "566  20.13  28.25  131.2  1261.0   0.0978  0\n",
       "567   16.6  28.08  108.3   858.1  0.08455  0\n",
       "568   20.6  29.33  140.1  1265.0   0.1178  0\n",
       "569   7.76  24.54  47.92   181.0  0.05263  1\n",
       "\n",
       "[569 rows x 6 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "df = pd.read_csv(filepath_or_buffer='Breast_cancer_data.csv', header=None)\n",
    "df = df.drop([0], axis=0) #drop first row (header names) from dataframe\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df[5]) #label encode column index 5 (target attribute)\n",
    "df[5] = le.transform(df[5]) #normalize target based on label encoding (into integer form)\n",
    "labels = df.iloc[:,5].values #labels: list of all values in column\n",
    "#df = df.drop([5], axis=1) #drop fifth column from dataframe\n",
    "samples = df.iloc[:,:5].values #2d array of all the samples (not including target variable)\n",
    "\n",
    "print('breast cancer data')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 samples, 5 attributes <class 'numpy.ndarray'>\n",
      "569\n",
      "455 455\n",
      "114\n"
     ]
    }
   ],
   "source": [
    "print(len(samples), \"samples,\", len(samples[0]), \"attributes\", type(samples))\n",
    "print(len(samples))\n",
    "trainsamples, testsamples = np.split(samples, [455]) #80/20 train/test split\n",
    "trainlabels, testlabels = np.split(labels, [455])\n",
    "print(len(trainsamples), len(trainlabels))\n",
    "print(len(testsamples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function that utilizes cross validation to test accuracy of model\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "def evaluate_model(model):\n",
    "    model.fit(trainsamples, trainlabels)\n",
    "    prediction = model.predict(testsamples)\n",
    "    scores = f1_score(testlabels, prediction, average='weighted') #return model's f1_score on test set\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.8598\n",
      "1 minimum samples per leaf, f1 score: 0.8598\n",
      "2 minimum samples per leaf, f1 score: 0.8214\n",
      "3 minimum samples per leaf, f1 score: 0.8606\n",
      "4 minimum samples per leaf, f1 score: 0.8921\n",
      "5 minimum samples per leaf, f1 score: 0.9080\n",
      "6 minimum samples per leaf, f1 score: 0.8921\n",
      "7 minimum samples per leaf, f1 score: 0.8685\n",
      "8 minimum samples per leaf, f1 score: 0.8685\n",
      "9 minimum samples per leaf, f1 score: 0.8685\n",
      "1 max depth, f1 score: 0.9298\n",
      "2 max depth, f1 score: 0.9298\n",
      "3 max depth, f1 score: 0.8528\n",
      "4 max depth, f1 score: 0.8842\n",
      "5 max depth, f1 score: 0.9080\n",
      "6 max depth, f1 score: 0.9080\n",
      "7 max depth, f1 score: 0.9080\n",
      "8 max depth, f1 score: 0.9080\n",
      "9 max depth, f1 score: 0.9080\n",
      "\n",
      "min_samples_leaf=5 , max_depth=1 achieve highest f1_score=.9298\n"
     ]
    }
   ],
   "source": [
    "#classic decision tree classifier (no ensemble method)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#default classifier, no tuning\n",
    "dtc = DecisionTreeClassifier(random_state=0)\n",
    "scores = evaluate_model(dtc)\n",
    "print('f1 score: {:.4f}'.format(scores.mean()))\n",
    "\n",
    "#tuning parameter min_samples_leaf\n",
    "for i in range(1,10):\n",
    "    dtc = DecisionTreeClassifier(random_state=0, min_samples_leaf=i)\n",
    "    scores = evaluate_model(dtc)\n",
    "    print(i, 'minimum samples per leaf, f1 score: {:.4f}'.format(scores.mean()))\n",
    "\n",
    "#min_samples_leaf=5 achieves highest f1 score\n",
    "#tuning hyperparameter max_depth\n",
    "for i in range(1,10):\n",
    "    dtc = DecisionTreeClassifier(random_state=0, min_samples_leaf=5, max_depth=i)\n",
    "    scores = evaluate_model(dtc)\n",
    "    print(i, 'max depth, f1 score: {:.4f}'.format(scores.mean()))\n",
    "\n",
    "print(\"\\nmin_samples_leaf=5 , max_depth=1 achieve highest f1_score=.9298\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.9167\n",
      "10 estimators, f1 score: 0.9160\n",
      "20 estimators, f1 score: 0.9080\n",
      "30 estimators, f1 score: 0.9080\n",
      "40 estimators, f1 score: 0.9080\n",
      "50 estimators, f1 score: 0.9080\n",
      "60 estimators, f1 score: 0.9247\n",
      "70 estimators, f1 score: 0.9167\n",
      "80 estimators, f1 score: 0.9167\n",
      "90 estimators, f1 score: 0.9167\n",
      "1 max features, f1 score: 0.9397\n",
      "2 max features, f1 score: 0.9328\n",
      "3 max features, f1 score: 0.9247\n",
      "4 max features, f1 score: 0.9167\n",
      "5 max features, f1 score: 0.9247\n",
      "\n",
      "n_estimators=60, max_features=1 achieve highest f1_score=.9397\n"
     ]
    }
   ],
   "source": [
    "#bagged decision tree\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "#draws from classic decision tree model, makes n_estimators\n",
    "\n",
    "#no tuning\n",
    "bc = BaggingClassifier(estimator=dtc, n_estimators=100, random_state=0)\n",
    "scores = evaluate_model(bc)\n",
    "print('f1 score: {:.4f}'.format(scores.mean()))\n",
    "\n",
    "#tuning n_estimators\n",
    "for i in range(1, 10):\n",
    "    bc = BaggingClassifier(estimator=dtc, n_estimators=i*10, random_state=0)\n",
    "    scores = evaluate_model(bc)\n",
    "    print(i*10, 'estimators, f1 score: {:.4f}'.format(scores.mean()))\n",
    "\n",
    "#best f1_score was w/ n_estimators=60\n",
    "#tune max_features hyperparameter\n",
    "for i in range(1,6):\n",
    "    bc = BaggingClassifier(estimator=dtc, n_estimators=60, random_state=0, max_features=i)\n",
    "    scores = evaluate_model(bc)\n",
    "    print(i, 'max features, f1 score: {:.4f}'.format(scores.mean()))\n",
    "\n",
    "print(\"\\nn_estimators=60, max_features=1 achieve highest f1_score=.9397\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.9167\n",
      "1 max depth, f1 score: 0.9390\n",
      "2 max depth, f1 score: 0.9315\n",
      "3 max depth, f1 score: 0.9160\n",
      "4 max depth, f1 score: 0.9087\n",
      "5 max depth, f1 score: 0.9167\n",
      "6 max depth, f1 score: 0.9087\n",
      "7 max depth, f1 score: 0.9167\n",
      "8 max depth, f1 score: 0.9167\n",
      "9 max depth, f1 score: 0.9080\n",
      "1 max features, f1 score: 0.9307\n",
      "2 max features, f1 score: 0.9390\n",
      "3 max features, f1 score: 0.9307\n",
      "4 max features, f1 score: 0.9144\n",
      "5 max features, f1 score: 0.9225\n",
      "\n",
      "max_depth=1, max_features=2 achieve highest f1_score=.9390\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#no tuning\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "scores = evaluate_model(rf)\n",
    "print('f1 score: {:.4f}'.format(scores.mean()))\n",
    "\n",
    "#tuning max_depth\n",
    "for i in range(1,10):\n",
    "    rf = RandomForestClassifier(random_state=0, max_depth=i)\n",
    "    scores = evaluate_model(rf)\n",
    "    print(i, 'max depth, f1 score: {:.4f}'.format(scores.mean()))\n",
    "\n",
    "#highest f1_score with max_depth=1\n",
    "#tuning max features\n",
    "for i in range(1,6):\n",
    "    rf = RandomForestClassifier(random_state=0, max_depth=1, max_features=i)\n",
    "    scores = evaluate_model(rf)\n",
    "    print(i, 'max features, f1 score: {:.4f}'.format(scores.mean()))\n",
    "\n",
    "print(\"\\nmax_depth=1, max_features=2 achieve highest f1_score=.9390\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.8992\n",
      "0.1 learning rate, f1 score: 0.9410\n",
      "0.2 learning rate, f1 score: 0.9160\n",
      "0.3 learning rate, f1 score: 0.9080\n",
      "0.4 learning rate, f1 score: 0.9072\n",
      "0.5 learning rate, f1 score: 0.9234\n",
      "0.6 learning rate, f1 score: 0.9153\n",
      "0.7 learning rate, f1 score: 0.9153\n",
      "0.8 learning rate, f1 score: 0.8992\n",
      "0.9 learning rate, f1 score: 0.9153\n",
      "10 estimators, f1 score: 0.9390\n",
      "20 estimators, f1 score: 0.9315\n",
      "30 estimators, f1 score: 0.9404\n",
      "40 estimators, f1 score: 0.9404\n",
      "50 estimators, f1 score: 0.9410\n",
      "60 estimators, f1 score: 0.9492\n",
      "70 estimators, f1 score: 0.9328\n",
      "80 estimators, f1 score: 0.9410\n",
      "90 estimators, f1 score: 0.9328\n",
      "\n",
      "learning_rate=0.1, n_estimators=60, achieves highest f1_score=.9492\n"
     ]
    }
   ],
   "source": [
    "#adaptive boost classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#no tuning\n",
    "ada = AdaBoostClassifier(random_state=0)\n",
    "scores = evaluate_model(ada)\n",
    "print('f1 score: {:.4f}'.format(scores.mean()))\n",
    "\n",
    "#tuning learning rate\n",
    "for i in range(1,10):\n",
    "    ada = AdaBoostClassifier(random_state=0, learning_rate=i/10)\n",
    "    scores = evaluate_model(ada)\n",
    "    print(i/10, 'learning rate, f1 score: {:.4f}'.format(scores.mean()))\n",
    "\n",
    "#best result was learning_rate=0.1, now tuning estimators\n",
    "for i in range(1,10):\n",
    "    ada = AdaBoostClassifier(random_state=0, learning_rate=0.1, n_estimators=10*i)\n",
    "    scores = evaluate_model(ada)\n",
    "    print(i*10, 'estimators, f1 score: {:.4f}'.format(scores.mean()))\n",
    "\n",
    "print(\"\\nlearning_rate=0.1, n_estimators=60, achieves highest f1_score=.9492\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ensemble of Ensembles\n",
    "\n",
    "Another way to create an ensemble is by combining the predictions of different models, e.g., the 4 different `best` models you found earlier. As usual, one has to decide how to combine the votes of each model. Read about the VotingClassifier on the [SKLearn documentation website](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html). You may also want to look at some tutorials online for using this ensemble model. Then train a VotingClassifier model that will use the 4 best models you found in step 1. Finally, use it to predict the samples in the test set and report the F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting test labels [1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 0\n",
      " 0 1 1 0 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
      " 0 0 1]\n",
      "actual test labels [1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1\n",
      " 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0\n",
      " 0 0 1]\n",
      "\n",
      "voting classifier achieves f1_score = 0.9307207994443325\n"
     ]
    }
   ],
   "source": [
    "#ensemble of ensembles, using voting classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=0, min_samples_leaf=5, max_depth=1) #optimal decision tree classifier\n",
    "bc = BaggingClassifier(estimator=dtc, n_estimators=60, random_state=0, max_features=1) #optimal bagging classifier\n",
    "rf = RandomForestClassifier(random_state=0, max_depth=1, max_features=2) #optimal random forest classifier\n",
    "ada = AdaBoostClassifier(random_state=0, learning_rate=0.1, n_estimators=60) #optimal adaptive booster classifier\n",
    "\n",
    "#create voting classifier with four estimators being the best models derived earlier\n",
    "voting = VotingClassifier(estimators=[ (\"dtc\", dtc), (\"bc\", bc), (\"rf\", rf), (\"ada\", ada) ], voting=\"hard\")\n",
    "\n",
    "#fit to training sample, predict test samples\n",
    "voting.fit(trainsamples, trainlabels)\n",
    "\n",
    "prediction = voting.predict(testsamples)\n",
    "print('predicting test labels', prediction)\n",
    "print('actual test labels', testlabels)\n",
    "\n",
    "score = f1_score(testlabels, prediction, average='weighted')\n",
    "print('\\nvoting classifier achieves f1_score =', score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
