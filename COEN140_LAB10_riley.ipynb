{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries and dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.datasets import load_digits\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "print(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torchvision.datasets.mnist.MNIST, 60000, torch.Size([60000, 28, 28]))"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#downloading the MNIST dataset\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "type(training_data), len(training_data), training_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize the labels map\n",
    "labels_map = {\n",
    "    0: \"0\",\n",
    "    1: \"1\",\n",
    "    2: \"2\",\n",
    "    3: \"3\",\n",
    "    4: \"4\",\n",
    "    5: \"5\",\n",
    "    6: \"6\",\n",
    "    7: \"7\",\n",
    "    8: \"8\",\n",
    "    9: \"9\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataloaders for the train and test set\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=1000, shuffle=True)\n",
    "\n",
    "type(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#network 1: applying 2d convolution, dropout2d, and linear transformation\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d() #flatten\n",
    "        self.fc1 = nn.Linear(320, 50) #320\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "network = Net()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.4680, -2.3241, -2.4465, -2.1292, -2.4854, -2.0555, -2.3951, -2.3723,\n",
      "         -2.2375, -2.2130]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alkal\\AppData\\Local\\Temp\\ipykernel_20276\\654680725.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "#check to see that it works on the input data we will pass in\n",
    "input = torch.randn(1, 1, 28, 28)\n",
    "out = network(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "torch.Size([10, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(network.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(0)\n",
    "\n",
    "network = Net()\n",
    "#using the stochastic gradient descent optimizer\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_dataloader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the network\n",
    "def train(epoch):\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    #loss function: negative log likelihood\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_dataloader.dataset),\n",
    "        100. * batch_idx / len(train_dataloader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_dataloader.dataset)))\n",
    "      #torch.save(network.state_dict(), 'results/model.pth')\n",
    "      #torch.save(optimizer.state_dict(), 'results/optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to test the network on the validation set\n",
    "def test():\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_dataloader:\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_dataloader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_dataloader.dataset),\n",
    "    100. * correct / len(test_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alkal\\AppData\\Local\\Temp\\ipykernel_20276\\654680725.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n",
      "c:\\Users\\alkal\\anaconda3\\envs\\c140\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3039, Accuracy: 987/10000 (10%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.297314\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.302465\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.323880\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.296098\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.311837\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.284443\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.308383\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.296021\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.301577\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.302344\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.290155\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.281338\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.298939\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.303516\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.269983\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.280778\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.280603\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.271497\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.287923\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.275012\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.244974\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.256999\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.248679\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.258569\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.225968\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.252222\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.201265\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.193116\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.193939\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.157874\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.166105\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 2.203985\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.124981\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 1.991860\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 1.855962\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 1.897462\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.946225\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 1.862774\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 1.621473\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 1.727456\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.587164\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 1.660731\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 1.626233\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 1.595927\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 1.377784\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 1.668928\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 1.433569\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 1.382896\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.388580\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 1.188935\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.054342\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 1.160104\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 1.073976\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 1.150542\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 1.107306\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 1.143756\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.962872\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.975977\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 1.118194\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.953137\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.131287\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.807095\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.811140\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.876904\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.786152\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.729063\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.707692\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 1.014614\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.794859\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.827224\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.975657\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.622573\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.831668\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.729978\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.781398\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.683960\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.791080\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.596902\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.756668\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.606368\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.622189\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.951715\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.662525\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.641563\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.691793\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.712474\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.755294\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.543893\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.451744\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.641507\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.769190\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.797689\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.612676\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.484302\n",
      "\n",
      "Test set: Avg. loss: 0.3520, Accuracy: 9013/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.623196\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.469641\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.572099\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.644617\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.765519\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.685526\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.655227\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.683981\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.638826\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.652261\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.528759\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.677841\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.624916\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.399407\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.598682\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.410584\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.619193\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.708096\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.600055\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.456473\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.504279\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.417540\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.507324\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.794062\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.647522\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.451746\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.408303\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.669643\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.737145\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.482030\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.336391\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.438358\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.674179\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.454162\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.605142\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.615323\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.730330\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.547639\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.563607\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.660998\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.578887\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.599048\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.413718\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.608931\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.437033\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.483800\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.441398\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.598590\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.603863\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.393008\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.312075\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.380034\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.619050\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.481826\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.688431\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.548557\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.505744\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.585141\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.413282\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.590603\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.357664\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.537600\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.542476\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.577529\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.473921\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.490826\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.484256\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.414006\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.683532\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.492598\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.477611\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.468337\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.347413\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.518581\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.505629\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.599943\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.407099\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.398298\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.409828\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.674443\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.424233\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.453563\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.416321\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.408923\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.439245\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.616733\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.503478\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.578810\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.680775\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.469061\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.868875\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.377270\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.523340\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.283392\n",
      "\n",
      "Test set: Avg. loss: 0.1928, Accuracy: 9413/10000 (94%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.479278\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.560144\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.429598\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.286678\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.503945\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.368854\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.431517\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.385774\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.404439\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.306260\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.446774\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.530779\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.474325\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.580019\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.405722\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.292137\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.295919\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.787142\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.418745\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.312934\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.439122\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.521403\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.455096\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.452854\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.309306\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.576471\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.385869\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.744945\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.459426\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.529610\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.443958\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.423756\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.284327\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.506542\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.296980\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.421239\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.316754\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.474724\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.322186\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.418507\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.532287\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.408853\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.437960\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.463069\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.387446\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.319027\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.328788\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.619959\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.361943\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.500755\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.297321\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.330056\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.296638\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.271739\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.374751\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.415504\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.307744\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.315526\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.375537\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.385660\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.309025\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.227099\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.375296\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.243612\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.740636\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.408828\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.422219\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.392331\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.401654\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.550468\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.305885\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.320895\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.390300\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.263962\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.433954\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.630186\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.244583\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.354498\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.173998\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.371721\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.435264\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.314579\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.210022\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.574330\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.310765\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.180889\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.160325\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.247429\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.768592\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.229755\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.688660\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.329011\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.365398\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.204035\n",
      "\n",
      "Test set: Avg. loss: 0.1455, Accuracy: 9552/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)\n",
    "  test() #after 3 epochs we achieve accuracy of 96%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network 2: using only linear transformations with rectified linear activation function\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.linear1 = nn.Linear(784, 50)\n",
    "        self.linear2 = nn.Linear(50, 50)\n",
    "        self.linear3 = nn.Linear(50, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.linear1(x))  \n",
    "        x = torch.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.0690, -4.7742, -0.7595, -0.9946, -5.2384, -4.7804, -6.5613, -3.1249,\n",
      "         -2.5937, -4.2882]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alkal\\AppData\\Local\\Temp\\ipykernel_20276\\654680725.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    }
   ],
   "source": [
    "#check to see that the network works properly on the input shape we pass in\n",
    "input = torch.randn(1, 1, 28, 28)\n",
    "out = network(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "def train(model, criterion, train_loader, validation_loader, optimizer, epochs=100):\n",
    "    i = 0\n",
    "    useful_stuff = {'training_loss': [], 'validation_accuracy': []}  \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            z = model(x.view(-1, 28 * 28))\n",
    "            loss = criterion(z, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            useful_stuff['training_loss'].append(loss.data.item())\n",
    "        \n",
    "        correct = 0\n",
    "        for x, y in validation_loader:\n",
    "            z = model(x.view(-1, 28 * 28))\n",
    "            _, label = torch.max(z, 1)\n",
    "            correct += (label == y).sum().item()\n",
    "    \n",
    "        accuracy = 100 * (correct / len(test_data))\n",
    "        useful_stuff['validation_accuracy'].append(accuracy)\n",
    "    \n",
    "    return useful_stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_epochs = 10 #number of iterations\n",
    "\n",
    "#loss function: cross entropy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#use stochastic gradient descent\n",
    "learning_rate = 0.01\n",
    "modelRelu = Net2()\n",
    "optimizer = torch.optim.SGD(modelRelu.parameters(), lr=learning_rate)\n",
    "training_results_relu = train(modelRelu, criterion, train_dataloader, test_dataloader, optimizer, epochs=cust_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[77.57,\n",
       " 87.59,\n",
       " 89.36,\n",
       " 90.29,\n",
       " 91.07,\n",
       " 91.81,\n",
       " 92.10000000000001,\n",
       " 92.39,\n",
       " 92.69,\n",
       " 93.12]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return accuracy of second network on the test set after each epoch (iteration)\n",
    "#after ten epochs the model achieves 93% accuracy\n",
    "training_results_relu['validation_accuracy']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c140",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
