{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**project 1**\n",
    "\n",
    "1. choose appropriate techniques for modelling text\n",
    "2. implement k-nearest neighbor classifier\n",
    "3. use k-NN classifier to assign classes to short texts\n",
    "4. evaluate results using F1-scoring metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import string\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "#import necessary modules\n",
    "\n",
    "print(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102080\n",
      "towns celebrate and hope amir can strike gold amir khans victory sparked scenes of celebration tonight in the two towns where the boxing sensation lives and trains family and friends in bolton and bury heaped praise on the\n",
      "\n",
      "['towns', 'celebrate', 'and', 'hope', 'amir', 'can', 'strike', 'gold', 'amir', 'khans', 'victory', 'sparked', 'scenes', 'of', 'celebration', 'tonight', 'in', 'the', 'two', 'towns', 'where', 'the', 'boxing', 'sensation', 'lives', 'and', 'trains', 'family', 'and', 'friends', 'in', 'bolton', 'and', 'bury', 'heaped', 'praise', 'on', 'the']\n",
      "music groups launch test case against website the music industry on monday launched a test case against kazaa one of the world 39s largest filesharing websites in a bid to stem rampant internet piracy\n",
      "\n",
      "['music', 'groups', 'launch', 'test', 'case', 'against', 'website', 'the', 'music', 'industry', 'on', 'monday', 'launched', 'a', 'test', 'case', 'against', 'kazaa', 'one', 'of', 'the', 'world', '39s', 'largest', 'filesharing', 'websites', 'in', 'a', 'bid', 'to', 'stem', 'rampant', 'internet', 'piracy']\n"
     ]
    }
   ],
   "source": [
    "#first we want to read all the data and put into a sparse matrix\n",
    "\n",
    "#read lines from 'train.dat' into 'traindata'\n",
    "#data becomes list of str\n",
    "with open(\"train.dat\", \"r\") as fh:\n",
    "    traindata = fh.readlines()\n",
    "\n",
    "#cls is empty set to keep track of which lines are which class\n",
    "cls = []\n",
    "\n",
    "#preprocessing train.dat: store class in cls\n",
    "#remove the class from beginning of lines for data processing\n",
    "for i in range(len(traindata)):\n",
    "    cls.append(traindata[i][0])\n",
    "    #print(cls[i])\n",
    "    traindata[i] = traindata[i][2:]\n",
    "\n",
    "print(len(cls))\n",
    "\n",
    "#preprocessing train lines\n",
    "for i in range(len(traindata)):\n",
    "    #preprocessing: convert to lowercase\n",
    "    traindata[i] = traindata[i].lower()\n",
    "    #remove punctuation\n",
    "    traindata[i] = traindata[i].translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "print(traindata[0])\n",
    "\n",
    "traindocs = [l.split() for l in traindata]\n",
    "print(traindocs[0])\n",
    "\n",
    "\n",
    "#read lines from 'test.dat' to 'testdata'\n",
    "with open(\"test.dat\", \"r\") as fh:\n",
    "    testdata = fh.readlines()\n",
    "\n",
    "#preprocessing test lines\n",
    "for i in range(len(testdata)):\n",
    "    #preprocessing: convert to lowercase\n",
    "    testdata[i] = testdata[i].lower()\n",
    "    #remove punctuation\n",
    "    testdata[i] = testdata[i].translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "print(testdata[0])\n",
    "\n",
    "testdocs = [l.split() for l in testdata]\n",
    "print(testdocs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 ['towns', 'celebrate', 'hope', 'amir', 'strike', 'gold', 'amir', 'khans', 'victory', 'sparked', 'scenes', 'celebration', 'tonight', 'towns', 'where', 'boxing', 'sensation', 'lives', 'trains', 'family', 'friends', 'bolton', 'bury', 'heaped', 'praise']\n",
      "23 ['music', 'groups', 'launch', 'test', 'case', 'against', 'website', 'music', 'industry', 'monday', 'launched', 'test', 'case', 'against', 'kazaa', 'world', 'largest', 'filesharing', 'websites', 'stem', 'rampant', 'internet', 'piracy']\n",
      "music groups launch test case against website music industry monday launched test case against kazaa world largest filesharing websites stem rampant internet piracy\n"
     ]
    }
   ],
   "source": [
    "#filter out words shorter than \"minlen\"\n",
    "def filterLen(document, minlen):\n",
    "    return[ [t for t in d if len(t) >= minlen] for d in document]\n",
    "\n",
    "\n",
    "#filter words shorter than 4 characters long\n",
    "traindocs = filterLen(traindocs, 4)\n",
    "testdocs = filterLen(testdocs, 4)\n",
    "\n",
    "print(len(traindocs[0]), traindocs[0])\n",
    "print(len(testdocs[0]), testdocs[0])\n",
    "\n",
    "for i in range(len(testdocs)):\n",
    "    testdata[i] = ' '.join(testdocs[i])\n",
    "\n",
    "print(testdata[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build matrix with all words in train.dat\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "def build_matrix(docs):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "    nrows = len(docs)\n",
    "    idx = {}\n",
    "    tid = 0\n",
    "    nnz = 0\n",
    "    for d in docs:\n",
    "        nnz += len(set(d))\n",
    "        for w in d:\n",
    "            if w not in idx:\n",
    "                idx[w] = tid\n",
    "                tid += 1\n",
    "    ncols = len(idx)\n",
    "        \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=int)\n",
    "    val = np.zeros(nnz, dtype=np.double)\n",
    "    ptr = np.zeros(nrows+1, dtype=int)\n",
    "    i = 0  # document ID / row counter\n",
    "    n = 0  # non-zero counter\n",
    "    # transfer values\n",
    "    for d in docs:\n",
    "        cnt = Counter(d)\n",
    "        keys = list(k for k,_ in cnt.most_common())\n",
    "        l = len(keys)\n",
    "        for j,k in enumerate(keys):\n",
    "            ind[j+n] = idx[k]\n",
    "            val[j+n] = cnt[k]\n",
    "        ptr[i+1] = ptr[i] + l\n",
    "        n += l\n",
    "        i += 1\n",
    "            \n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n",
    "\n",
    "\n",
    "def csr_info(mat, name=\"\", non_empy=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empy, \n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)) )\n",
    "\n",
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "    If copy is True, returns a copy of the normalized matrix.\n",
    "    \"\"\"\n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    # normalize\n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue  # do not normalize empty rows\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [nrows 102080, ncols 89963, nnz 2240750]\n"
     ]
    }
   ],
   "source": [
    "#building the matrix\n",
    "mat = build_matrix(traindocs)\n",
    "csr_info(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-nearest neighbor algorithm\n",
    "def findNearestNeighbors(name, i=0, k=1):\n",
    "    # first, find the document for the given name\n",
    "    id = i\n",
    "    # compute similarities of name's vector against all other name vectors\n",
    "    csr_l2normalize(mat)\n",
    "    x = mat[id,:]\n",
    "    dots = x.dot(mat.T)\n",
    "    dots[0,id] = -1 # invalidate self-similarity\n",
    "    sims = list(zip(dots.indices, dots.data))\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [s[0] for s in sims[:k] if s[1] > 0 ]\n",
    "    return [traindocs[s[0]] for s in sims[:k] if s[1] > 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[175], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39moutput.dat\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m fh:\n\u001b[0;32m      2\u001b[0m     \u001b[39m#for i in range(5):\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(testdata)):\n\u001b[1;32m----> 4\u001b[0m         s \u001b[39m=\u001b[39m findNearestNeighbors(testdata[i], i, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m         fh\u001b[39m.\u001b[39mwrite(\u001b[39mcls\u001b[39m[s[\u001b[39m0\u001b[39m]])\n\u001b[0;32m      6\u001b[0m         fh\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[172], line 6\u001b[0m, in \u001b[0;36mfindNearestNeighbors\u001b[1;34m(name, i, k)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mid\u001b[39m \u001b[39m=\u001b[39m i\n\u001b[0;32m      5\u001b[0m \u001b[39m# compute similarities of name's vector against all other name vectors\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m csr_l2normalize(mat)\n\u001b[0;32m      7\u001b[0m x \u001b[39m=\u001b[39m mat[\u001b[39mid\u001b[39m,:]\n\u001b[0;32m      8\u001b[0m dots \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mdot(mat\u001b[39m.\u001b[39mT)\n",
      "Cell \u001b[1;32mIn[165], line 77\u001b[0m, in \u001b[0;36mcsr_l2normalize\u001b[1;34m(mat, copy, **kargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m     rsum \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\u001b[39m/\u001b[39mnp\u001b[39m.\u001b[39msqrt(rsum)\n\u001b[0;32m     76\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ptr[i], ptr[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]):\n\u001b[1;32m---> 77\u001b[0m         val[j] \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m rsum\n\u001b[0;32m     79\u001b[0m \u001b[39mif\u001b[39;00m copy \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m mat\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(\"output.dat\", \"w\") as fh:\n",
    "    #for i in range(5):\n",
    "    for i in range(len(testdata)):\n",
    "        s = findNearestNeighbors(testdata[i], i, 1)\n",
    "        fh.write(cls[s[0]])\n",
    "        fh.write('\\n') #546\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
